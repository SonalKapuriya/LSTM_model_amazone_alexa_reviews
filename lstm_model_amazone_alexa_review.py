# -*- coding: utf-8 -*-
"""LSTM_model_amazone_alexa_review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sGMDcEd-JQ5HVJxtkWhuRgrs9QzTB9Vb
"""

# To open any kaggle file in google colab
#1.Create profile in kaggle 
#2.go to kaggle account and there you will find "crate api token" thats it create new api using that 
#3.after that you will get one "kaggle.json" named file upload that into your in your session storage(temporary) of google colab notebook
#4now run following commands
#note:kindly check what is json file,what is kaggle api token and what is session storage.


!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import RandomOverSampler# to do oversampling for imbalanced dataset

!kaggle datasets download -d sid321axn/amazon-alexa-reviews

# to unzip dog vs cat file
import zipfile 
zip_ref=zipfile.ZipFile("/content/amazon-alexa-reviews.zip","r")
zip_ref.extractall("/content")
zip_ref.close()

df=pd.read_csv('/content/amazon_alexa.tsv',sep='\t')
df.head()

df.shape
df["feedback"].value_counts()

print(np.unique(np.array(df["feedback"]),return_counts=True))# highly imbalanced data

df_x=df.drop(["variation","feedback","date","rating"],axis=1)
df_y=df["feedback"]
oversample = RandomOverSampler(sampling_strategy='minority')# lets blance the dataset as following
df_x,df_y = oversample.fit_resample(df_x,df_y)
#print(df_y.value_counts())
#print(df_x.shape)
#We have found some punctuations marks,we have to remove that first

import string
#print(string.punctuation)# all punctuations
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree
#storing the puntuation free text
df['verified_reviews']= df['verified_reviews'].apply(lambda x:remove_punctuation(x))
# lowering the text
df['verified_reviews']=df['verified_reviews'].apply(lambda x:x.lower())
#talkenization of text
#df['verified_reviews']=df['verified_reviews'].apply(lambda x:x.split())# will not apply as we want to apply countvectorizer on it

import nltk
nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('english')

from sklearn.model_selection import GridSearchCV,train_test_split

X_train,X_test,Y_train,Y_test = train_test_split(df_x,df_y,random_state=6)
xtrain,X_cv,ytrain,Y_cv = train_test_split(X_train,Y_train,random_state=6)

# from sklearn.preprocessing import OneHotEncoder
# enc = OneHotEncoder(handle_unknown='ignore')
# one_hot_encoded_variation_train=enc.fit_transform(xtrain["variation"].values.reshape(-1,1))
# one_hot_encoded_variation_cv=enc.transform(X_cv["variation"].values.reshape(-1,1))

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(max_df=50,min_df=3,stop_words=stopwords)
xtrain.verified_reviews.sample(3)

v=vectorizer.fit_transform(xtrain["verified_reviews"])
p=vectorizer.transform(X_cv["verified_reviews"])
r=vectorizer.transform(X_test["verified_reviews"])
#to convert new data in Dataframe
train_verified_reviews=pd.DataFrame(v.toarray(),columns=vectorizer.get_feature_names_out())
cv_verified_reviews=pd.DataFrame(p.toarray(),columns=vectorizer.get_feature_names_out())
test_verified_reviews=pd.DataFrame(r.toarray(),columns=vectorizer.get_feature_names_out())
train_verified_reviews.head()
cv_verified_reviews.head()

# from scipy.sparse import coo_matrix, hstack
# # to concatenate new features got by vectorizer applyed on text feature
# final_train=hstack([train_verified_reviews,one_hot_encoded_variation_train])

# final_cv=hstack([cv_verified_reviews,one_hot_encoded_variation_cv])

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(class_weight="balanced")

# hyperparameter tuuining
#Experimenting with different value of C which is regularization term here.
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,log_loss
cv=[]
log_loss_cv=[]
for i in [0.001,0.01,0.1,1,2,3]:
  clf = LogisticRegression(C=i)
  clf.fit(train_verified_reviews,ytrain)
  cv.append(accuracy_score(Y_cv,clf.predict(cv_verified_reviews)))
  log_loss_cv.append(log_loss(Y_cv,clf.predict_proba(cv_verified_reviews)))

train=[]
log_loss_train=[]
for i in [0.001,0.01,0.1,1,2,3]:
  clf = LogisticRegression(C=i)
  clf.fit(train_verified_reviews,ytrain)
  train.append(accuracy_score(ytrain,clf.predict(train_verified_reviews)))
  log_loss_train.append(log_loss(ytrain,clf.predict_proba(train_verified_reviews)))

plt.plot([0.001,0.01,0.1,1,2,3],cv)
plt.plot([0.001,0.01,0.1,1,2,3],train)
plt.title("accuracy of train and test set")
plt.xlabel("value of C in logistic regression")
plt.ylabel("accuracy")
plt.legend(["test","train"])

plt.plot([0.001,0.01,0.1,1,2,3],log_loss_cv)
plt.plot([0.001,0.01,0.1,1,2,3],log_loss_train)
plt.title("logloss of train and cv set")
plt.xlabel("value of C in logistic regression")
plt.ylabel("log_loss")
plt.legend(["cv","train"])

#Lets take value of C=0.1
clff = LogisticRegression(class_weight="balanced")
clf.fit(train_verified_reviews,ytrain)

# # lets find loss,accuracy and confusin matrix for test data set
# one_hot_encoded_variation_test=enc.transform(X_test["variation"].values.reshape(-1,1))
# f=vectorizer.transform(X_test["verified_reviews"])

# test_verified_reviews=pd.DataFrame(f.toarray(),columns=vectorizer.get_feature_names_out())

# final_test=hstack([test_verified_reviews,one_hot_encoded_variation_test])

y_log_test=clf.predict_proba(test_verified_reviews)

test_predicted=clf.predict(test_verified_reviews)
accuracy_score(test_predicted,Y_test)

from sklearn.metrics import confusion_matrix
confusion_matrix(Y_test,test_predicted)

Y_test.value_counts()



import tensorflow 
import keras
from keras.preprocessing.text import Tokenizer
tokenizer=Tokenizer(num_words=5000)# max length of list of words will be = num_words 
tokenizer.fit_on_texts(xtrain["verified_reviews"])
tokenized_train = tokenizer.texts_to_sequences(xtrain["verified_reviews"])
tokenized_cv = tokenizer.texts_to_sequences(X_cv["verified_reviews"])
tokenized_test = tokenizer.texts_to_sequences(X_test["verified_reviews"])

len(tokenized_train),

all_words_in_train=tokenizer.word_counts.keys()
index_all_words_in_train=tokenizer.word_index

len(tokenizer.word_counts)# there are total 1593 unique wordss

#tokenizer.word_docs 
#Again a dictionary of words, this tells us how many documents contain this word

#tokenizer.word_index
#In this dictionary, we have unique integers assigned to each word.

print(tokenizer.document_count) # 
print(xtrain.shape)
#This integer count will tell us the total number of documents used for fitting the tokenizer.

# # lets understand it by example
# import pandas as pd
# D_train=pd.DataFrame({"text":["this is my pen","this is my pet dog","this dog"]})
# D_test=pd.DataFrame({"text":["this is my pen","this is any my","this pen is mine"]})
# Tokenizer1=Tokenizer(num_words=6)
# Tokenizer1.fit_on_texts(D_train["text"])
# Tokenized_Dtrain=Tokenizer1.texts_to_sequences(D_train["text"])
# Tokenized_=Tokenizer1.texts_to_sequences(D_train["text"])

# Tokenized_Dtest=Tokenizer1.texts_to_sequences(D_test["text"])

# Tokenizer1.word_counts

# Tokenizer1.word_docs

# Tokenizer1.word_index

# Tokenized_Dtrain

# Tokenized_Dtest

# lets move to our task
# apply padding to overcome the problem of having all input of same size

from keras_preprocessing.sequence import pad_sequences
maxlen=500
tokenized_train_paded = pad_sequences(tokenized_train, maxlen=maxlen)
tokenized_cv_paded = pad_sequences(tokenized_cv, maxlen=maxlen)
tokenized_test_paded = pad_sequences(tokenized_test, maxlen=maxlen)

tokenized_train_paded.shape# all sentence now as vector of 500 dimention

!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove*.zip

embeddings_index = {}
f = open('glove.6B.100d.txt', encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))

f = open('glove.6B.100d.txt', encoding='utf-8')
for line in f:
  values = line.split()
  print(values)
  word = values[0]
  print(word)
  coefs = np.asarray(values[1:], dtype='float32')
  print(coefs)
  embeddings_index[word] = coefs
  print(coefs.shape)
  break

print(len(embeddings_index))
embeddings_index["apple"].shape

all_words_in_train=tokenizer.word_counts.keys()
index_all_words_in_train=tokenizer.word_index
dictt={}
for word in all_words_in_train:
  if word in embeddings_index.keys():
    dictt[index_all_words_in_train[word]]=embeddings_index[word]
  else:
    dictt[word]=np.zeros(100,)

len(all_words_in_train)

len(index_all_words_in_train)

len(dictt)

vocab_size=len(tokenizer.word_index) + 1
embedding_matrix = np.zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

#That means we got vector for each word in vocaulaty of text of our data set

embedding_matrix.shape,

dictt[1].shape

embedding_matrix[1].shape

# lets create sequencial model
from keras import Sequential
from keras.layers import Embedding,LSTM,Dense
max_features=len(tokenizer.word_index)+1
embed_size=100
maxlen=500# all vector of sentence will be of 500 in length
model = Sequential()
model.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=False))
#LSTM 
model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))
model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))
model.add(Dense(units = 32 , activation = 'relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer=keras.optimizers.Adam( learning_rate= 0.01), loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(tokenized_train_paded, ytrain, batch_size = 256 , validation_data = (tokenized_cv_paded,Y_cv) , epochs = 10)

epochs = [i for i in range(10)]
fig , ax = plt.subplots(1,2)
train_acc = history.history['accuracy']
train_loss = history.history['loss']
val_acc = history.history['val_accuracy']
val_loss = history.history['val_loss']
fig.set_size_inches(20,10)

ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')
ax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')
ax[0].set_title('Training & Testing Accuracy')
ax[0].legend()
ax[0].set_xlabel("Epochs")
ax[0].set_ylabel("Accuracy")

ax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')
ax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')
ax[1].set_title('Training & Testing Loss')
ax[1].legend()
ax[1].set_xlabel("Epochs")
ax[1].set_ylabel("Loss")
plt.show()

score = model.evaluate(tokenized_test_paded, Y_test)

pred_prob = model.predict(tokenized_test_paded)
pred=np.where(pred_prob<0.5,0,1)

Y_test

pred_prob

from sklearn.metrics import confusion_matrix,accuracy_score
confusion_matrix(Y_test,pred) 
#accuracy_score(Y_test,pred)

print(classification_report(Y_test, pred))

